Generate imagesThe Gemini API supports image generation using Gemini 2.0 Flash Experimental and using Imagen 3. This guide helps you get started with both models.

Before you begin



Before calling the Gemini API, ensure you have your SDK of choice installed, and a Gemini API key configured and ready to use.

Generate images using Gemini



Gemini 2.0 Flash Experimental supports the ability to output text and inline images. This lets you use Gemini to conversationally edit images or generate outputs with interwoven text (for example, generating a blog post with text and images in a single turn). All generated images include a SynthID watermark, and images in Google AI Studio include a visible watermark as well.

Note: Make sure to include responseModalities: ["Text", "Image"] in your generation configuration for text and image output with gemini-2.0-flash-exp-image-generation. Image only is not allowed.

The following example shows how to use Gemini 2.0 to generate text-and-image output:

Python

JavaScript

REST



from google import genaifrom google.genai import typesfrom PIL import Imagefrom io import BytesIOimport base64client = genai.Client()contents = ('Hi, can you create a 3d rendered image of a pig '

            'with wings and a top hat flying over a happy '

            'futuristic scifi city with lots of greenery?')response = client.models.generate_content(

    model="gemini-2.0-flash-exp-image-generation",

    contents=contents,

    config=types.GenerateContentConfig(

      response_modalities=['Text', 'Image']

    ))for part in response.candidates[0].content.parts:

  if part.text is not None:

    print(part.text)

  elif part.inline_data is not None:

    image = Image.open(BytesIO((part.inline_data.data)))

    image.save('gemini-native-image.png')

    image.show()

AI-generated image of a fantastical flying pig

Depending on the prompt and context, Gemini will generate content in different modes (text to image, text to image and text, etc.). Here are some examples:

Text to imageExample prompt: "Generate an image of the Eiffel tower with fireworks in the background."

Text to image(s) and text (interleaved)Example prompt: "Generate an illustrated recipe for a paella."

Image(s) and text to image(s) and text (interleaved)Example prompt: (With an image of a furnished room) "What other color sofas would work in my space? can you update the image?"

Image editing (text and image to image)Example prompt: "Edit this image to make it look like a cartoon"

Example prompt: [image of a cat] + [image of a pillow] + "Create a cross stitch of my cat on this pillow."

Multi-turn image editing (chat)Example prompts: [upload an image of a blue car.] "Turn this car into a convertible." "Now change the color to yellow."

Image editing with Gemini



To perform image editing, add an image as input. The following example demonstrats uploading base64 encoded images. For multiple images and larger payloads, check the image input section.

Python

JavaScript

REST



from google import genaifrom google.genai import typesfrom PIL import Imagefrom io import BytesIOimport PIL.Imageimage = PIL.Image.open('/path/to/image.png')client = genai.Client()text_input = ('Hi, This is a picture of me.'

            'Can you add a llama next to me?',)response = client.models.generate_content(

    model="gemini-2.0-flash-exp-image-generation",

    contents=[text_input, image],

    config=types.GenerateContentConfig(

      response_modalities=['Text', 'Image']

    ))for part in response.candidates[0].content.parts:

  if part.text is not None:

    print(part.text)

  elif part.inline_data is not None:

    image = Image.open(BytesIO(part.inline_data.data))

    image.show()

Limitations

For best performance, use the following languages: EN, es-MX, ja-JP, zh-CN, hi-IN.

Image generation does not support audio or video inputs.

Image generation may not always trigger:The model may output text only. Try asking for image outputs explicitly (e.g. "generate an image", "provide images as you go along", "update the image").

The model may stop generating partway through. Try again or try a different prompt.

When generating text for an image, Gemini works best if you first generate the text and then ask for an image with the text.

Choose a model



Which model should you use to generate images? It depends on your use case.

Gemini 2.0 is best for producing contextually relevant images, blending text + images, incorporating world knowledge, and reasoning about images. You can use it to create accurate, contextually relevant visuals embedded in long text sequences. You can also edit images conversationally, using natural language, while maintaining context throughout the conversation.

If image quality is your top priority, then Imagen 3 is a better choice. Imagen 3 excels at photorealism, artistic detail, and specific artistic styles like impressionism or anime. Imagen 3 is also a good choice for specialized image editing tasks like updating product backgrounds, upscaling images, and infusing branding and style into visuals. You can use Imagen 3 to create logos or other branded product designs.

Generate images using Imagen 3



The Gemini API provides access to Imagen 3, Google's highest quality text-to-image model, featuring a number of new and improved capabilities. Imagen 3 can do the following:

Generate images with better detail, richer lighting, and fewer distracting artifacts than previous models

Understand prompts written in natural language

Generate images in a wide range of formats and styles

Render text more effectively than previous models

Note: Imagen 3 is only available on the Paid Tier and always includes a SynthID watermark.

Python

JavaScript

REST



from google import genaifrom google.genai import typesfrom PIL import Imagefrom io import BytesIOclient = genai.Client(api_key='GEMINI_API_KEY')response = client.models.generate_images(

    model='imagen-3.0-generate-002',

    prompt='Robot holding a red skateboard',

    config=types.GenerateImagesConfig(

        number_of_images= 4,

    ))for generated_image in response.generated_images:

  image = Image.open(BytesIO(generated_image.image.image_bytes))

  image.show()

AI-generated image of two fuzzy bunnies in the kitchen

Imagen supports English only prompts at this time and the following parameters:

Imagen model parameters



(Naming conventions vary by programming language.)

numberOfImages: The number of images to generate, from 1 to 4 (inclusive). The default is 4.

aspectRatio: Changes the aspect ratio of the generated image. Supported values are "1:1", "3:4", "4:3", "9:16", and "16:9". The default is "1:1".

personGeneration: Allow the model to generate images of people. The following values are supported:"DONT_ALLOW": Block generation of images of people.

"ALLOW_ADULT": Generate images of adults, but not children. This is the default.



8. the tool calling can be enabled or disabled in the current conversation too 

Function Calling with the Gemini APIFunction calling lets you connect models to external tools and APIs. Instead of generating text responses, the model understands when to call specific functions and provides the necessary parameters to execute real-world actions. This allows the model to act as a bridge between natural language and real-world actions and data. Function calling has 3 primary use cases:

Augment Knowledge: Access information from external sources like databases, APIs, and knowledge bases.

Extend Capabilities: Use external tools to perform computations and extend the limitations of the model, such as using a calculator or creating charts.

Take Actions: Interact with external systems using APIs, such as scheduling appointments, creating invoices, sending emails, or controlling smart home devices




